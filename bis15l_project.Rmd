---
title: "bis15l_project"
author: "Sidney Parel"
date: "2/25/2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
library(tidyverse)
library(janitor)
library(lubridate)

library(maps)
library(ggmap)
library(gganimate)
library(gifski)
library(transformr)

library(ggsci)
library(paletteer)

library(ggwordcloud)
library(tm)
```

## Load the U.S. clinical cases data sets.
The reported clinical cases data sets for the years 2016 - 2021 contain the 
same variables but do not have the same column names. Therefore, before merging 
these data sets, we standardized the column names and removed missing values.
```{r, message = FALSE}
## Load the reported clinical cases (rcc) data sets:
# Obtain file names.
rcc_files <- list.files(path = "Data/us_clinical_cases", 
                        pattern = ".csv", 
                        full.names = TRUE)

# Store all data sets in a list.
rcc <- rcc_files %>% 
  lapply(read_csv)

# Rename the data sets by year.
rcc_names <- list.files(path = "Data/us_clinical_cases", 
                        pattern = ".csv") %>% 
  strsplit(".csv") %>% 
  unlist()

names(rcc) <- rcc_names

# Check if the number of columns is the same in each data set.
# for (i in 1:(length(rcc) - 1)){
  # print(dim(rcc[[i]])[2] == dim(rcc[[i + 1]])[2])}
    # number of cols in 2019 daa does not match 2020 data
    # number of cols in 2020 does not match 2021 data

# check the columns in the 2020 data set.
# rcc$reported_cases_2020 %>% 
#   colnames()
    # Empty columns

# Remove empty columns in the 2020 data set.
rcc$reported_cases_2020 <- rcc$reported_cases_2020 %>% 
  select(1:4)

# Check that number of columns match in all data sets after removing the empty 
# columns in the 2020 data set.
# for (i in 1:(length(rcc) - 1)){
#  print(dim(rcc[[i]])[2] == dim(rcc[[i + 1]])[2])}
    # All data sets now have the same number of columns

# Check that the column names match across all data sets.
# for (i in 1:(length(rcc) - 1)){
#  print(colnames(rcc[[i]]) == colnames(rcc[[i + 1]]))}
  # The columns contain the same data types, but the names do not match
  # across all data sets.
```

```{r}
## Standardize the column names and variable types:
# Create a vector containing the new names.
rcc_col_names <- c("jurisdiction", "any_cases", "clinical_cases", "range")

# Assign the new names to all data sets.
rcc <- rcc %>% 
  lapply(setNames, rcc_col_names)

# Change the character column variable types to factor.
# for loop solution
for (i in 1:length(rcc)){
  rcc[[i]] <- rcc[[i]] %>% 
    mutate(across(where(is.character), factor))
}
# purr solution?


## Add a year column to all data sets:
# Obtain all years in the list of data sets.
rcc_years <- rcc_names %>% 
  str_remove("reported_cases_") %>% 
  as.integer()

# Add the year to all rows.
# for loop solution
# for (i in 1:length(rcc)){
#  rcc[[i]] <- rcc[[i]] %>% 
#    mutate(year = rcc_years[i])}

# purr solution
rcc <- rcc %>%
  map2(rcc_years, ~mutate(.x, year = .y))
```

```{r}
# Merge the data sets:
# Drop any rows with non-clinical case counts.
all_reported_cases <- rcc %>% 
  bind_rows() %>% 
  relocate(year) %>% 
  arrange(year) %>% 
  filter(!is.na(jurisdiction),
         clinical_cases != 0) %>% 
  select(-any_cases) 

# write_csv(all_reported_cases, "all_reported_cases.csv")
```


## Load the NCBI Isolates Browser data.
```{r, message = FALSE}
# Load the NCBI isolates data and select the variables of interest:
# Load the data.
ncbi <- read_csv("Data/ncbi_isolates.csv") %>% 
  clean_names() %>% 
  select(isolate, create_date, location, 
         isolation_source, isolation_type, snp_cluster)

# Select all rows associated with US clinical cases.
ncbi_clinical_isolates <- ncbi %>%
  filter(str_detect(location, "USA"),
         isolation_type == "clinical",) %>% 
  drop_na()

# Remove "USA: " and "USA:" from values in the location column 
# and rename as state.
ncbi_clinical_isolates <- ncbi_clinical_isolates %>% 
  mutate(location = str_replace(location, "USA: ", "")) %>% 
  mutate(location = str_replace(location, "USA:", "")) %>% 
  mutate(location = str_replace(location, "Houston", "Texas")) %>% 
  mutate(location = str_replace(location, "Chicago", "Illinois")) %>% 
  mutate(location = str_replace(location, "New jersey", "New Jersey")) %>% 
  filter(location != "USA") %>% 
  rename(state = location) %>% 
  mutate(state = factor(state)) 
  
# Change the values in the isolation source column to one of two categories: 
# blood and other.
ncbi_clinical_isolates <- ncbi_clinical_isolates %>% 
  mutate(isolation_source = 
           case_when(str_detect(isolation_source, "blood") ~ "blood",
                     TRUE ~ "other"))

# Change the remaining character columns to factor.
ncbi_clinical_isolates <- ncbi_clinical_isolates %>% 
  mutate(across(where(is.character), factor))

#write_csv(ncbi_clinical_isolates, "ncbi_clinical_isolates.csv")
```


## Create an animated map to show the clinical cases in the US since 2016.
```{r}
## Create an animated map for the reported clinical cases:
# Load the state boundary basemap.
states <- map_data("state.vbm") %>% 
  tibble() %>% 
  mutate(region = factor(region))

USplot <- ggplot() +
  geom_polygon(data = states, aes(x=long, y = lat, group = group)) 

# Join the reported clinical cases data to the state boundary map data.
#   rcc data has state abbreviations, not names
#   need to add a state name column to the rcc data
#     make a new data frame using built in state name data sets
state_key <- tibble(jurisdiction = state.abb, region = state.name)
all_reported_cases <- inner_join(all_reported_cases, 
                                 state_key, 
                                 by = "jurisdiction") %>% 
  mutate(region = factor(region))

# Join the reported clinical cases data to the state boundary map data.
statewide_cases <- inner_join(states, all_reported_cases, by = "region")

# Since some states had no cases, they will appear as empty polygons in a map
# Need to create an additional data frame containing the names of states
# without cases for each year so that we can fill in the empty polygons

# Create the not in operator.
`%!in%` <- Negate(`%in%`)

# Subset statewide reported cases by year.
for (i in 1:6) {
  assign(paste("statewide_cases", rcc_years[i], sep = "_"), 
         statewide_cases %>% 
           filter(year == rcc_years[i]))
}

statewide_cases_list <- list(statewide_cases_2016,
                             statewide_cases_2017,
                             statewide_cases_2018,
                             statewide_cases_2019,
                             statewide_cases_2020,
                             statewide_cases_2021)

# Use the statewide case count year subsets to extract states 
# with no cases for each year.
for (i in 1:6) {
  assign(paste("no_cases", rcc_years[i], sep = "_"), 
         state_key %>% 
           filter(region %!in% statewide_cases_list[[i]]$region) %>%  
           select(region) %>% 
           mutate(year = rcc_years[i]))
}

no_cases <- bind_rows(no_cases_2016,
                      no_cases_2017,
                      no_cases_2018,
                      no_cases_2019,
                      no_cases_2020,
                      no_cases_2021)

# Join the extracted states to the state map data to create the data frame
# containing all states with no cases for each year.
no_cases <- inner_join(states, no_cases, by = "region")

# Draw the geographical map.
fig <- ggplot() +
  geom_polygon(data = no_cases, 
               aes(x = long, y = lat, group = group), fill = "gray") +
  geom_polygon(data = statewide_cases, 
               aes(x = long, y = lat, group = group, fill = clinical_cases)) +
  labs(fill = "Count") +
  scale_fill_viridis_c(option = "mako", direction = -1) +
  theme_void()

# Animate the map and save as a gif.
fig_animated <- fig +
  transition_time(year) +
  ggtitle('Clinical Cases of Candida auris in {frame_time}') +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold",
                                  margin = margin(t = 15, b = -15)),
        legend.box.margin = margin(10,10,10,10))

animate(fig_animated, nframes = 6, fps = 0.5, height = 450, width = 600)
#anim_save("us_clinical_cases_map.gif")
```
## Visualize the NCBI isolate data.
```{r}
## Calculate the proportion of isolates from blood for each SNP cluster as an
# indicator of bloodstream infection.
isolation_sources <- ncbi_clinical_isolates %>% 
  group_by(snp_cluster) %>% 
  count(isolation_source)

cluster_sources <- ncbi_clinical_isolates %>% 
  group_by(snp_cluster) %>% 
  tabyl(snp_cluster,isolation_source) %>% 
  arrange(desc(blood))

ggplot(data = isolation_sources) +
  geom_col(aes(x = fct_reorder(snp_cluster, n),
               y = n,
               fill = isolation_source)) +
  labs(x = "SNP Cluster",
       y = "Count",
       fill = "Isolation Source") +
  coord_flip() +
  scale_fill_viridis_d(direction = -1) +
  theme_minimal()

# SNP clusters associated with highest proportion of blood isolates are 
# PDS000050611.11, PDS000050610.18, and PDS000050696.2
```

```{r}
# Find states with snp_clusters with the highest prop of bloodstream infections.
blood_clusters <- ncbi_clinical_isolates %>% 
  filter(isolation_source == "blood",
         snp_cluster %in% as.vector(head(cluster_sources$snp_cluster, 3)))

blood_clusters %>% 
 group_by(state) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))

```

## Visualize the Google Trends data.
```{r, message = FALSE}
## Create word clouds for the related queries:
# Load the related queries data.
related <- read_csv("Data/google_search_trends/
                    searchterm_candidaauris/relatedQueries.csv", 
                    skip = 3,
                    col_names = TRUE)

# Separate top queries from rising queries.
top_related <- related %>% 
  head(25)

rising_related <- related %>% 
  tail(26) %>%
  rename(query = TOP) %>% 
  slice(-1) %>% 
  mutate(query = str_remove(query, ",Breakout"))

## Obtain counts for words in top queries:
# Create Separate columns for counts and queries.
top_counts <- top_related %>% 
  separate(TOP, c("query", "count"), ",") %>% 
  head(-2) # remove candida and auris

# Use text mining to get total counts for each word.
# Create and preprocess a corpus for top queries.
top_corpus <- Corpus(VectorSource(top_counts$query)) %>% 
  tm_map(removeWords, c("is", "of", "candida", "auris"))

# Create a document term matrix from the corpus.
# Each document (query) is represented by a set of tokens (words) and their counts.
top_dtm <- TermDocumentMatrix(top_corpus) %>% 
  as.matrix()

# Get word frequencies from the dtm.
top_words <- rowSums(top_dtm) 
top_totals <- tibble(word = names(top_words), freq = top_words) 
# freq > 1 = duplicate words
# multiply freq by count for each word

# Combine word frequencies with query counts to get totals for each word.
top_totals <- top_totals %>% 
  mutate(count = c((100+14), 27, (25+8), 20, 19, 17, 17, 
                   16, 14, (12+10), 10, 9, 8, 8)) %>% 
  mutate(n = freq*count)

## Create the word cloud.
set.seed(1)
ggplot(data = top_totals) +
  geom_text_wordcloud(aes(label = word , size = n, color = word)) +
  scale_size_area(max_size = 28) +
  scale_color_manual(values = paletteer_c("grDevices::Teal", 14)) +
  theme_minimal()
```

```{r, message = FALSE}
## Obtain counts for words in rising queries:
# No count column, so word frequencies are the totals.
# Create and preprocess a corpus for rising queries.
rising_corpus <- Corpus(VectorSource(rising_related$query)) %>% 
  tm_map(removeWords, c("is", "of", "candida", "auris"))

# create a document term matrix from the corpus.
rising_dtm <- TermDocumentMatrix(rising_corpus) %>% 
  as.matrix()

# Get word frequencies from the dtm.
rising_words <- rowSums(rising_dtm) 
rising_totals <- tibble(word = names(rising_words), freq = rising_words) 

## create the word cloud
ggplot(data = rising_totals) +
  geom_text_wordcloud(aes(label = word, size = freq, color = word)) +
  scale_size_area(max_size = 18) +
  scale_color_manual(values = paletteer_c("grDevices::Teal", 17)) +
  theme_minimal()
```

## Compare number of reported clinical cases and Google searches in 2021.
```{r}
## Create the 2021 reported clinical cases map:
# Extract center coordinates for each state from the state boundary map data:
center_coords <- state.vbm.center %>% 
  as.data.frame() %>% 
  tibble() %>% 
  rename(long = x,
         lat = y) %>% 
  mutate(region = unique(states$region)) %>% 
  relocate(region)

# Join 2021 statewide case counts with center coordinates
# to use as state labels:
# Select region and case count columns from the 2021 data frame.
# Join columns with the center coordinates.
# Join with state abbreviations for faster filtering.
coords_2021 <- statewide_cases_2021 %>% 
  group_by(region) %>% 
  count(clinical_cases) %>% 
  select(-n) %>% 
  left_join(center_coords, by = "region") %>% 
  left_join(state_key, by = "region") %>% 
  rename(abb = jurisdiction)

# Crete the 2021 reported cases map:
ggplot() +
  geom_polygon(data = no_cases %>% 
                 filter(year == 2021), 
               aes(x = long, y = lat, group = group), fill = "gray") +
  geom_polygon(data = statewide_cases_2021, 
               aes(x = long, y = lat, group = group, fill = clinical_cases)) +
  geom_label(data = coords_2021,
            aes(x = long, y = lat, label = clinical_cases),
            fontface = "bold") +
  labs(fill = "Count",
       title = "Reported Clinical Cases of Candida auris in 2021") +
  theme_void() +
  scale_fill_viridis_c(option = "mako", direction = -1) +
  theme(plot.title = element_text(size = 14,
                                  margin = margin(t = 15, b = -15)),
        legend.margin = margin(10, 30, 10, 10))
  

# No cases in Oregon but high freq of searches from google trends data
# voluntary reporting?
```

```{r, message = FALSE}
## Create the 2021 Google searches map.
# Load the Google searches data.
searches_2021 <- read_csv("Data/google_search_trends/
                          searchterm_candidaauris/geoMap_2021.csv",
                          skip = 2,
                          col_names = TRUE) %>% 
  rename(region = Region,
         count = `Candida auris: (3/9/21 - 3/9/22)`)

# Add the abbreviation column for faster filtering.
searches_2021 <- inner_join(searches_2021, states, by = "region") %>% 
  inner_join(state_key, by = "region") %>% 
  rename(abb = jurisdiction) %>% 
  relocate(abb, .before = count)

# Relate the center coordinates to the number of searches in each state.
searches_2021_coords <- inner_join(searches_2021 %>% 
                                     select(region, abb, count),
                                   center_coords,
                                   by = "region")

# Create a vector containing the abbreviations of states of interest.
# Includes states with high number of reported cases or google searches.
states_of_interest <- c("CA", "IL", "NY", "FL", "OR")
soi_2021_coords <- searches_2021_coords %>% 
  filter(abb %in% states_of_interest)

ggplot(data = searches_2021) +
  geom_polygon(aes(x = long, y = lat, group = group, fill = count)) +
  geom_label(data = soi_2021_coords,
             aes(x = long, y = lat, label = count),
             fontface = "bold") +
  theme_void() +
  scale_fill_viridis_c(option = "mako", direction = -1, na.value = "gray") +
  labs(fill = "Count",
       title = 'Number of Google Searches Containing "Candida auris" in 2021') +
  theme(plot.title = element_text(size = 14,
                                  margin = margin(t = 15, b = -15)),
        legend.margin = margin(10, 30, 10, 10))
```

```{r}
# Make a copy of the 2021 reported cases map with only the states of interest
# labeled.
ggplot() +
  geom_polygon(data = no_cases %>% 
                 filter(year == 2021), 
               aes(x = long, y = lat, group = group), fill = "gray") +
  geom_polygon(data = statewide_cases_2021, 
               aes(x = long, y = lat, group = group, fill = clinical_cases)) +
  geom_label(data = coords_2021 %>% 
               filter(abb %in% states_of_interest),
            aes(x = long, y = lat, label = clinical_cases),
            fontface = "bold") +
  theme_void() +
  labs(fill = "Count",
       title = "Reported Clinical Cases of Candida auris in 2021") +
  scale_fill_viridis_c(option = "mako", direction = -1) +
  theme(plot.title = element_text(size = 14,
                                  margin = margin(t = 15, b = -15)),
        legend.margin = margin(10, 30, 10, 10))
   
```

